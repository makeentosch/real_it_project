# -*- coding: utf-8 -*-
"""chat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FyrJQUHWWyibXit7f4bn59GyS-uZPkc9

# Препроцессинг данных

Импорт библиотек
"""

from google.colab import files
import io
import pandas as pd
import json
import matplotlib.pyplot as plt
import numpy as np

"""Загрузка "сырых" чатов"""

uploaded = files.upload()

"""Формирование списка чатов из json-файла"""

filename = list(uploaded.keys())[0]
json_str = io.StringIO(uploaded[filename].decode('utf-8')).read()
data = json.loads(json_str)
data = pd.json_normalize(data['clients'] )
chats = data['chats']
chats

"""Создание пустого датафрейма"""

df = pd.DataFrame(columns=['client', 'operator'])
df

"""Добавление диалогов из списка всех чатов в датафрейм"""

for chat in chats:
  if isinstance(chat, list):
    for dialog in chat:
      events = dialog['events']
      client_msgs = []
      operator_msgs = []

      # Если первое сообщение написал оператор, пропускаем его
      if events[0]['agent_id'] is not None:
        i = 1
      else:
        i = 0

      while i < len(events):
        # Добавляем сообщения клиента в список
        if events[i]['agent_id'] is None:
          client_msgs.append([])
          j = i
          while j < len(events) and events[j]['agent_id'] is None:
            client_msgs[-1].append(events[j]['text'])
            j += 1
          i = j

        # Добавляем сообщения оператора в список
        else:
          operator_msgs.append([])
          j = i
          while j < len(events) and events[j]['agent_id'] is not None:
            operator_msgs[-1].append(events[j]['text'])
            j += 1
          i = j

      # Добавляем сообщения в датафрейм
      df = pd.concat([df, pd.DataFrame({'client': [client_msgs], 'operator': [operator_msgs]})], ignore_index=True)

len(df) # Количество всех диалогов

"""Выявление диалогов без ответа оператора или клиента"""

empty_dialogs = df[(df['client'].apply(len) == 0) | (df['operator'].apply(len) == 0)]
empty_dialogs.head(10)

len(empty_dialogs) # Количество таких диалогов равно 1068, что очень много, учитывая, что у нас всего 2853 диалогов

"""Удаление "пустых" диалогов"""

df = df.drop(empty_dialogs.index)
len(df) # Теперь у нас осталось 1785 диалогов

"""Анализ продолжительности диалогов (продолжительность диалога складывается из количества сообщений, отправленных клиентом и оператором в рамках одного диалога)"""

client_lengths = df["client"].apply(lambda x: len([len(i) for i in x]))
operator_lengths = df["operator"].apply(lambda x: len([len(i) for i in x]))

# Создаем индексы для столбцов
indices = np.arange(len(df))

# Устанавливаем размер диаграммы
plt.figure(figsize=(20, 10))  # Размер в дюймах

# Рисуем столбцы
plt.bar(indices - 0.2, client_lengths, 0.4, label="Клиент")
plt.bar(indices + 0.2, operator_lengths, 0.4, label="Оператор")

# Добавляем подписи
plt.xlabel("Диалоги")
plt.ylabel("Количество сообщений")
plt.legend()

# Показываем график
plt.show()

"""Исходя из графика, можно заметить, что по-настоящему информативных диалогов очень мало

/

Преобразование датафрейма с диалогами в датафрейм, содержащий данные в формате вопрос-ответ. Эти данные в последствии будут использованы для обучения модели нейросети
"""

curr_client_phrases = df['client']
curr_operator_phrases = df['operator']
client_phrases = []
operator_phrases = []

for client_phrase_list, operator_phrase_list in zip(curr_client_phrases, curr_operator_phrases):
    for client_phrase, operator_phrase in zip(client_phrase_list, operator_phrase_list):
        client_phrases.append(client_phrase)
        operator_phrases.append(operator_phrase)

df = pd.DataFrame({'client': client_phrases, 'operator': operator_phrases})

df.tail(100)

len(df) # Имеем 4592 пары вопросов - ответов

"""Удаление вопросов, состоящих из одного слова (считаются неинформативными)"""

def is_single_word(arr):
  if len(arr) == 1:
    if arr[0] is not None and len(arr[0].split()) == 1:
      return True
  return False

df = df[~df['client'].apply(is_single_word)]

len(df)

"""Пример того, в каком виде хранятся данные в текущем датафрейме. Необходимо избавиться от списка и склеить вопросы в один, удалить кавычки"""

df["client"][1472]

"""Преобразование вложенных списков в единый список и удаление кавычек"""

df["client"] = [" ".join(map(str, el)) for el in df["client"]]
df["operator"] = [" ".join(map(str, el)) for el in df["operator"]]

df["client"][1472]

"""Выгрузка датафрейма на компьютер"""

df.to_excel("data.xlsx", index=False)
files.download("data.xlsx")

"""Загрузка файла xlsx после ручного удаления спама и нерелевантных диалогов и преобразование его обратно в датафрейм"""

uploaded = files.upload()

for fn in uploaded.keys():
  df = pd.read_excel(fn)

df

"""Подготовка данных для нейросети"""

questions = df["client"].tolist()
answers = df["operator"].tolist()
print(len(questions) == len(answers))

questions[99]

"""# Нейронка (rut5-small)"""

!pip install transformers sentencepiece
!pip install accelerate -U

import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments

tokenizer = T5Tokenizer.from_pretrained("cointegrated/rut5-small-chitchat")
model = T5ForConditionalGeneration.from_pretrained("cointegrated/rut5-small-chitchat")

# Вопрос пользователя
user_question = "Здравствуйте! Есть ли у вас летний лагерь для ребенка 13 лет по программированию?"

# Ответ из базы данных
db_answer = "Лагеря нет"

# Формирование входных данных для модели
input_text = f"Вопрос: {user_question} Ответ: {db_answer}"

# Кодирование входных данных и генерация ответа
inputs = tokenizer.encode(input_text, return_tensors='pt')
outputs = model.generate(inputs, max_length=40, num_beams=5, early_stopping=True)
decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(decoded_output)

# Токенизация
train_encodings = tokenizer(train_client, truncation=True, padding=True)
train_labels = tokenizer(train_operator, truncation=True, padding=True)

# Создание датасета
class ChatDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # Мы используем input_ids в качестве меток
        return item

    def __len__(self):
        return len(self.encodings.input_ids)

train_dataset = ChatDataset(train_encodings, train_labels)

# Обучение
training_args = TrainingArguments(
    output_dir='./results',          # выходной каталог
    num_train_epochs=3,              # количество эпох обучения
    per_device_train_batch_size=8,  # размер батча для обучения
    per_device_eval_batch_size=32,   # размер батча для оценки
    warmup_steps=500,                # количество шагов для разогрева
    weight_decay=0.01,               # степень затухания весов
    logging_dir='./logs',            # каталог для логирования
)

trainer = Trainer(
    model=model,                         # модель для обучения
    args=training_args,                  # аргументы обучения
    train_dataset=train_dataset,         # датасет для обучения
)

trainer.train()



"""# Нейронка RuGPT-3"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("sberbank-ai/rugpt3large_based_on_gpt2")
model = GPT2LMHeadModel.from_pretrained("sberbank-ai/rugpt3large_based_on_gpt2")

# # Ваш контекст и данные
# context = "Наличие лагеря"
# data = ["Есть", 50000,  "01.07-13.07", "Екатеринбург"]

# # Формирование входного текста для модели
# input_text = context + ": " + ", ".join(map(str, data))
prompt = "Сообщи клиенту, что лагерь имеется, и его стоимость 10000 рублей"

# Генерация ответа
inputs = tokenizer.encode(prompt, return_tensors='pt')
outputs = model.generate(inputs, max_length=150, temperature=0.7, num_return_sequences=1)

# Декодирование и вывод ответа
response  = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response )

"""# Нейронка GPT-2"""

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

prompt = "Сообщи клиенту, что лагерь имеется, и его стоимость 10000 рублей"
inputs = tokenizer.encode(prompt, return_tensors='pt')

outputs = model.generate(inputs, max_length=150, temperature=0.7, num_return_sequences=1)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(response)